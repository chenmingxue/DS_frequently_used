{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data loading\n# 2. Data investigation and cleaning\n# 3. Data visualization and engineering\n# 4. Data preparation for modeling\n# 5. Repeat 2-4 until get satisfied results"},{"metadata":{},"cell_type":"markdown","source":"# file read"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read from JSON\ndef load_df(csv_path='../input/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n\n    df = pd.read_csv(csv_path, dtype={'fullVisitorId': 'str'}, nrows=nrows)\n\n    for column in JSON_COLUMNS:\n        df = df.join(pd.DataFrame(df.pop(column).apply(pd.io.json.loads).values.tolist(), index=df.index))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data aggregation and merging"},{"metadata":{"trusted":true},"cell_type":"code","source":"gp_fullVisitorId_train = train.groupby(['fullVisitorId']).agg('sum')\ngp_fullVisitorId_train['fullVisitorId'] = gp_fullVisitorId_train.index\ngp_fullVisitorId_train['mean_hits_per_day'] = gp_fullVisitorId_train.groupby(['day'])['totals.hits'].transform('mean')\ngp_fullVisitorId_train['mean_pageviews_per_day'] = gp_fullVisitorId_train.groupby(['day'])['totals.pageviews'].transform('mean')\ngp_fullVisitorId_train['sum_hits_per_day'] = gp_fullVisitorId_train.groupby(['day'])['totals.hits'].transform('sum')\ngp_fullVisitorId_train['sum_pageviews_per_day'] = gp_fullVisitorId_train.groupby(['day'])['totals.pageviews'].transform('sum')\ngp_fullVisitorId_train = gp_fullVisitorId_train[['fullVisitorId', 'mean_hits_per_day', 'mean_pageviews_per_day', 'sum_hits_per_day', 'sum_pageviews_per_day']]\ntrain = train.join(gp_fullVisitorId_train, on='fullVisitorId', how='inner', rsuffix='_')\ntrain.drop(['fullVisitorId_'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_agg = train.groupby('date')['totals.transactionRevenue'].agg(['count', 'sum'])\nyear_agg = train.groupby('year')['totals.transactionRevenue'].agg(['sum'])\nmonth_agg = train.groupby('month')['totals.transactionRevenue'].agg(['sum'])\nday_agg = train.groupby('day')['totals.transactionRevenue'].agg(['sum'])\nweekday_agg = train.groupby('weekday')['totals.transactionRevenue'].agg(['count','sum'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,7))\nplt.ticklabel_format(axis='y', style='plain')\nplt.ylabel('Sum transactionRevenue', fontsize=12)\nplt.xlabel('Date', fontsize=12)\nplt.scatter(time_agg.index.values, time_agg['sum'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## one hot encoding and spiliting"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"df_train_2 = pd.get_dummies(df_train, columns = ['wheezy-copper-turtle-magic'], prefix='wctm-')\ndf_test_2 = pd.get_dummies(df_test, columns = ['wheezy-copper-turtle-magic'], prefix='wctm-')\nX_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(df_train_2.drop(columns = ['id','target']),df_train_2['target'], test_size=0.15, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['device.isMobile','year', 'month', 'weekday', 'day']\ntrain = pd.get_dummies(train, columns=categorical_features)\ntest = pd.get_dummies(test, columns=categorical_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## correlation and plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corr = train.iloc[:,:50].drop([\"target\", 'id'], axis=1).corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(train_corr, vmin=-0.016, vmax=0.016, cmap=\"RdYlBu_r\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## feature selection - method\nIf there are multiple numeric columns and have different set of std, can have different levels in groups, to select useful columns, do modeling with mutitple (two) sets and observe the behavior, remove the weak ones. (reduce irrelative columns)\nhttps://scikit-learn.org/stable/modules/feature_selection.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"#method 1\nsel = VarianceThreshold(threshold=1.5).fit(train2[cols])\ntrain3 = sel.transform(train2[cols])\n#method 2 PCA\nStandardScaler().fit_transform(PCA(svd_solver='full',n_components='mle', whiten = 'True').fit_transform(data[cols]))\n# Larsso, etc.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## feature selection - 2\nPCA: reduce variables in order mainly to fix overfitting, if there is no overfitting, it will have worse result as information lost\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## time conversion"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_threshold = datetime.date(2019, 5, 29) #a single datetime.date point\ndate_threshold=pd.to_datetime(date_threshold) #convert to timestamp in order to compare with timestamp from pandas\nrank_data['date'] = pd.to_datetime(rank_data['SubmissionDate']) #pandas convert str of time to timestamp\nrank_data_0529 = rank_data[rank_data['date'] > date_threshold] #pandas column filter by time\n\ndef add_time_features(df):\n    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='ignore')\n    df['year'] = df['date'].apply(lambda x: x.year)\n    df['month'] = df['date'].apply(lambda x: x.month)\n    df['day'] = df['date'].apply(lambda x: x.day)\n    df['weekday'] = df['date'].apply(lambda x: x.weekday())\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# create new dataframe\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta = pd.DataFrame(data, columns = ['feature_name', 'col_name_1', 'col_name_2', 'col_name_3', 'col_name_4'])\nmeta.set_index('feature_name', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## data visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"#multiple histogram overlap\nplt.hist(rank_data_0529['Score'], bins=50, alpha=0.5, label='29', range=(0.9, 0.99)) \nplt.hist(rank_data_0528['Score'], bins=50, alpha=0.5, label='28', range=(0.9, 0.99)) \nplt.hist(rank_data_0527['Score'], bins=50, alpha=0.5, label='27', range=(0.9, 0.99))\nplt.legend(loc='upper right')\nplt.title('5/27-29')\nplt.xlabel('auc')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#overlay\nplt.hist([data_arrays], bins=50, alpha=0.5, label=[names], range=(0.9, 0.99)) \nplt.legend(loc='upper right')\nplt.title('5/27-29')\nplt.xlabel('auc')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# print out results"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('MSE: %.2f' % mse)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# prepare data for modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get labels\nY_train = X_train['totals.transactionRevenue'].values\nY_val = X_val['totals.transactionRevenue'].values\nX_train = X_train.drop(['totals.transactionRevenue'], axis=1)\nX_val = X_val.drop(['totals.transactionRevenue'], axis=1)\ntest = test.drop(['totals.transactionRevenue'], axis=1)\n# Log transform the labels\nY_train = np.log1p(Y_train)\nY_val = np.log1p(Y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modeling","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## stacking\nStacking (also called meta ensembling) is a model ensembling technique used to combine information from multiple predictive models to generate a new model.\nFor this reason, stacking is most effective when the base models are significantly different.\nhttp://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/"},{"metadata":{"trusted":true},"cell_type":"code","source":"## LGBM\nparams = {\n\"objective\" : \"regression\",\n\"metric\" : \"rmse\", \n\"num_leaves\" : 500,\n\"min_child_samples\" : 20,\n\"learning_rate\" : 0.005,\n\"bagging_fraction\" : 0.6,\n\"feature_fraction\" : 0.7,\n\"bagging_frequency\" : 1,\n\"bagging_seed\" : 1,\n\"lambda_l1\": 3,\n'min_data_in_leaf': 70\n}\nlgb_train = lgb.Dataset(X_train, label=Y_train)\nlgb_val = lgb.Dataset(X_val, label=Y_val)\nmodel = lgb.train(params, lgb_train, 10000, valid_sets=[lgb_train, lgb_val], early_stopping_rounds=100, verbose_eval=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make prediction on validation data.\nval_predictions = model.predict(X_val, num_iteration=model.best_iteration) #validation\n# Get min and max values of the predictions and labels.\nmin_val = max(max(val_predictions), max(Y_val))\nmax_val = min(min(val_predictions), min(Y_val))\n# Create dataframe with validation predicitons and labels.\nval_df = pd.DataFrame({\"Label\":Y_val})\nval_df[\"Prediction\"] = val_predictions\n# Plot data\nsns.set(style=\"darkgrid\")\nsns.jointplot(y=\"Label\", x=\"Prediction\", data=val_df, kind=\"reg\", color=\"m\", height=10)\nplt.plot([min_val, max_val], [min_val, max_val], 'm--')\nplt.show()\n#feature importance\nlgb.plot_importance(model, figsize=(15, 10))\nplt.show()\n\nval_predictions[val_predictions<0] = 0\nmse = mean_squared_error(val_predictions, Y_val)\nrmse = np.sqrt(mean_squared_error(val_predictions, Y_val))\nprint('Model validation metrics')\nprint('MSE: %.2f' % mse)\nprint('RMSE: %.2f' % rmse)\n#prediction\npredictions = model.predict(test, num_iteration=model.best_iteration)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}